{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Assignment"
      ],
      "metadata": {
        "id": "sexwqUWQC-QN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "ANS: It comes under supervised machine learning algorithms.\n",
        "- To establish or relationship between the  two variable or more than two variable.\n",
        "- To learns from the labelled datasets and maps the data points with most optimized linear function which can be used for prediction on new datasets.\n"
      ],
      "metadata": {
        "id": "GHu1K-enDBQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "ANS: There are 5 main key assumptions of Simple Linear Regression:\n",
        "1) Linearity: The relationship between the independent variable and the dependent variable must be linear.\n",
        "2) Independence: The observations should be independent of each other.\n",
        "3) Homoscedasticity: The variance of residual should remain constant across all values of X. If the spread of residuals increases or decreases , it indicates heteroscedasticity, which can lead to unreliable predictions.\n",
        "4) Normality of Residuals: Residuals should be normally distributed.\n",
        "5) No outliers: Extreme values can distort the model."
      ],
      "metadata": {
        "id": "paN_2gJND2uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c\n",
        "ANS: The coefficient m represent in the equation Y=mX+c represents the slope of the regression line. It indicates the rate of change of the dependent variable for a one-unit increase in the independent variable."
      ],
      "metadata": {
        "id": "yjFUL8CiGITE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "ANS: The intercept c represent in the equation Y=mX+c represents the value of Y when X = 0. It is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "- It gives the expected value of Y when the independent variable X= 0.\n",
        "- If c is positive , the regression line starts above the origin.\n",
        "- If c is negative , the regression line starts below the origin."
      ],
      "metadata": {
        "id": "oW9aF4r9GpfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "ANS: The slope m in the equation Y=mX+c is calculated using the Least Squares method, which minimizes the sum of squared errors between actual and predicted values.\n",
        "m = ∑(Xi- X)(Yi-Y) / ∑(Xi- X)2\n",
        "\n",
        "XiYi = Individual data points\n",
        "X = Mean of X\n",
        "Y = Mean of Y\n",
        "\n",
        "steps:\n",
        "1) Compute the mean of X and Y\n",
        "2) Compute the numertaor.\n",
        "3) Compute the denominator\n",
        "4) Divide the numertaor by the denominotor to get m .\n"
      ],
      "metadata": {
        "id": "1YtL9U61HhsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is the purpose of the least squares method in Simple Linear Regression?\n",
        "ANS: The least squares method is used in Simple Linear Regression to find the best-fit line that minimizes the error between the actual and predicted values of the dependent variable.\n",
        "- It ensures that the chosen regression line has the smallest possible sum of squared differences between actual and predicted values.\n",
        "- It helps in finding the optimal slope and intercept of the regression equation: Y=mX+c\n",
        "- It reduces the impact of positive and negative errors by squaring the residuals."
      ],
      "metadata": {
        "id": "4NLeNS8uI-zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "ANS: The coefficient of determination (R²) is a statistical measure that explains how well the independent variable predicts the dependent variable in a regression model.\n",
        "\n",
        "Interpretation of R²:\n",
        "1) R² = 1 : All data points lie exactly on the regression line.\n",
        "2) R² = 0 : The model is no better than using the mean as a prediction.\n",
        "3) R² = 0.80 : The model explains 80% of the variance in Y, and the remaining 20% is due to unexplained factors.\n",
        "4) R² < 0.5 : The independent variable X only explains a small portion of Y's variation."
      ],
      "metadata": {
        "id": "1Q_Fejh5KdbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "ANS: It involve multiple independent variables to predict a single dependent variable."
      ],
      "metadata": {
        "id": "ETCo_Hp3MESq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is the main difference between Simple and Multiple Linear Regression?\n",
        "ANS: Simple Linear Regression:\n",
        "- Models a linear relationship between one feature and the target variable.\n",
        "- Y=mX+c\n",
        "- It has one predictor.\n",
        "- When only one factor affects the outcome.\n",
        "- E.g: Predict the price of a house based on area of house.\n",
        "\n",
        "Multiple Linear Regression\n",
        "- Models the combined effect of multiple predictors in the target variable.\n",
        "- Y=b0+b1x1+b2x2+......+bnxn\n",
        "- It has two or more predictor.\n",
        "- When multiple factors influence the outcome.\n",
        "- E.g: Predict the price of a house based on area of house, location, Number of rooms.\n",
        "\n"
      ],
      "metadata": {
        "id": "nou49hitMYct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  10. What are the key assumptions of Multiple Linear Regression?\n",
        "  ANS: Multiple Linear Regression extends Simple linear regression by using two or more independent variables to predict a dependent variable.\n",
        "  The key assumptions of Multiple Linear Regression:\n",
        "  1) Linearity : The relationship between each independent variable and the dependent variable must be linear.\n",
        "  2) Independence: Observations are independent.\n",
        "  3) Homoscedasticity : The variance of  residuals should be constant across all values of X. If variance changes , predictions become unreliable.\n",
        "  4) No Multicollinearity : independent variables are not highly correlated.\n",
        "  5) Normality of Residuals: The residuals should follow a normal distribution for valid hypothesis testing.\n"
      ],
      "metadata": {
        "id": "ihSqjGEqOu1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "ANS: Heteroscedasticity occurs when the variance of residuals is not constant across all values of the independent variables. This violates the assumption of homoscedasticity, which states that errors should have constant variance.\n",
        "It affect the results of a Multiple Linear Regression model:\n",
        "1) Biased Standard Error : Confidence intervals and hypothesis test become unreliable.\n",
        "2) Inefficient Coefficients: OLS regression assumes constant variance , so heteroscedasticity makes coefficient unstable.\n",
        "3) Poor prediction Accuracy: Model predictions have inconsistent errors, making confidence intervals inaccurate."
      ],
      "metadata": {
        "id": "qhWIVU2wOupT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "ANS: Multicollinearity  occurs when two or more independent variables are highly correlated, making it difficult to determine their individual effects on the dependent variables. This leads to Unstable and inflated regression coefficients, Incorrect statistical significance, Poor model interpretability.\n",
        "To improve a Multiple Linear Regression model:\n",
        "1) Remove highly correlated predictors: Identify highly correlated variables using a correlation matrix or variance inflation factor.\n",
        "2) Principal component analysis: Reduce dimensionality by combining corelated features into uncorrelated principal components.\n",
        "3) Use Ridge Regression: Ridge Regression reduces the impact of correlated variables by shrinking coefficient values.\n",
        "4) Use Lasso Regression: Lasso Regression automatically removes less important variables by shrinking some coefficients to zero."
      ],
      "metadata": {
        "id": "bUOjPPr0OubQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "ANS: Categorical variables contains discrete values. Since regression models require numerical input, categorical data must be transformed into numerical representation before use.\n",
        "1) One-Hot Encoding: It is used for nominal categorical variables. It works well with models like linear regression.\n",
        "2) Label Encoding: It is used for ordinal categorical variables.\n",
        "3) Ordinal Encoding: It is used for ordinal categorical variables with defined hierarchy.\n",
        "4) Target Encoding: It is used for high cardinality categorical variables.\n",
        "5) Frequency Encoding: It compact representation for high cardinality data."
      ],
      "metadata": {
        "id": "0tgIejfrVNHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "ANS: Interaction terms capture the combined effect of two or more independent variables on the dependent variables . They are used when the relationship between one predictor and the target depends on another predictor.  Interaction terms help capture real world complexity. It use them when independent variables influence each other. Avoid unnecessary interactions to prevent overfitting."
      ],
      "metadata": {
        "id": "c8nmyQ56W7s3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "ANS: The intercept in regression models represents the predicted value of the dependent variable when all independent variables are zero. However, its interpretation differs in simple linear regression and multiple linear regression.\n",
        "Simple Linear Regression:\n",
        "- It expected Y when X =0.\n",
        "- In Simple Linear Regression, the intercept is easier to interpret but may not always be meaningful.\n",
        "\n",
        "Multiple Linear Regression:\n",
        "- It expected Y when all X's are zero.\n",
        "- In Multiple Linear Regression, the intercept often has no direct interpretation, especially when features cannot logically be zero."
      ],
      "metadata": {
        "id": "6qkHAqKVYcwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "ANS: The slope in regression analysis represents the rate of change of the dependent variable for a one-unit increase in the independent variable. It quantifies the strength and direction of the relation between X and Y.\n",
        "Positive Slope= X increases, Y increases\n",
        "Negative Slope= X increases, Y decreases\n",
        "Zero Slope= X has no effect on Y.\n",
        "A larger absolute slope makes Y more sensitive to changes in X.\n",
        "\n"
      ],
      "metadata": {
        "id": "nK4gXiNreN7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "ANS: The intercept in a regression model represents the predicted value of the dependent variable. When all independent variables are zero. It provides a baseline value  and helps in understanding how the dependent variable behaves when there are no influences from the independent variables.\n",
        "The intercept gives the straiting value of Y when all predictors are absent. The intercept can indicate whether a meaningful prediction exists at X= 0. The models with different independent variables can be compared by looking at  their intercepts. If an intercept is unreasonably high or low, it may suggest missing variables or the need for log transformation."
      ],
      "metadata": {
        "id": "tfis6swjfheV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What are the limitations of using R² as a sole measure of model performance?\n",
        "ANS: The coefficient of determination (R²) measures how well a regression model explains the variance in the dependent variable.\n",
        "R² = 1 - SSE/ SST\n",
        "R², or the coefficient of determination, is a useful metric for assessing how well a model explains the variance in the target variable.\n",
        "A high R² does not guarantee that the model makes accurate predictions on new, unseen data. It only reflects how well the model fits the training data, making it susceptible to overfitting.\n",
        "A model with more features may achieve a higher R² by overfitting to the training data, even if those features don't contribute meaningfully to predictions."
      ],
      "metadata": {
        "id": "PJtsk6kehZpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "ANS: A large standard error for a regression coefficient indicates a high level of uncertainty in the estimated value of that coefficient.\n",
        "- A large standard error suggests that the coefficient is imprecisely estimated, meaning the true value of the coefficient could vary widely from the point estimate provided by the regression model.\n",
        "- A larger standard error typically leads to wider confidence intervals for the coefficient, reducing your confidence in the estimated relationship between the predictor and the target variable.\n",
        "- If the standard error is large for one or more coefficients, it could indicate multicollinearity, meaning the predictor variables are highly correlated and not providing independent information to the model.\n"
      ],
      "metadata": {
        "id": "Na-PcDBmi4Xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "ANS: Heteroscedasticity occurs when the variability of the residuals is not constant across all levels of the independent variables.\n",
        "- Plot the residuals on the y-axis and the fitted values (predicted values) on the x-axis.\n",
        "- A horizontal line with random scatter suggests homoscedasticity (constant variance), while a systematic or widening pattern suggests heteroscedasticity.\n",
        "- It is important to address it because Heteroscedasticity violates one of the key assumptions of linear regression: that residuals have constant variance.\n",
        "- This can lead to biased estimates of standard errors, affecting t-tests, F-tests and confidence intervals.\n",
        "- Estimates of regression coefficients remain unbiased, but they may no longer be efficient. This means you could make less accurate predictions.\n",
        "-Heteroscedasticity can reduce the generalizability of your model, making it less reliable when applied to new data."
      ],
      "metadata": {
        "id": "Ltqt1UJHjbWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "ANS: If R² is high but Adjusted R²is low, it suggests that some independent variables in the model are not contributing meaningful information and may be unnecessary.\n",
        "- It typically indicates that the model may be overfitting or including irrelevant predictors.\n"
      ],
      "metadata": {
        "id": "Zp2gNfwhkgYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "ANS: Scaling variables in Multiple Linear Regression is important to improve numerical stability, interpretability, and model performance.\n",
        "- Regression models use coefficients to quantify relationships.\n",
        "- Variables on very different scales can cause numerical instability in the calculations of regression algorithms, particularly for iterative methods like gradient descent. This might result in convergence issues or imprecise estimates of coefficients.\n",
        "- Scaling can sometimes help reduce multicollinearity issues by ensuring that predictors have comparable ranges, thus improving the interpretability of the coefficients."
      ],
      "metadata": {
        "id": "gnTAK2X4lU1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "ANS: Where the relationship between the independent variable and the dependent variable. It is used when the data exhibits a non-linear relationship that cannot be accurately captured by simple linear regression."
      ],
      "metadata": {
        "id": "emQinkFwl9Uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  How does polynomial regression differ from linear regression?\n",
        "ANS: Polynomial regression and linear regression are related techniques, but they differ in how they model the relationship between the independent variables and the dependent variables.\n",
        "- In linear regression models a linear relationship between predictors and the outcome variable.\n",
        "- In polynomial regression models a nonlinear relationship by extending linear regression to include polynomial terms"
      ],
      "metadata": {
        "id": "u-pmWArmmdNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "ANS: Polynomial Regression is used when the relationship between the independent variable and the dependent variable is non-linear but can be approximated by a polynomial function.\n",
        "1)  When Data Shows a Non-Linear Pattern : polynomial model may be a better fit than a straight-line model.\n",
        "2) When a Straight Line Underfits the Data: Linear regression fails when the relationship is curved.\n",
        "3) When Domain Knowledge Suggests a Curved Relationship: Some real-world problems naturally follow polynomial patterns.\n",
        "4)  When Model Complexity Needs to Be Controlled: Polynomial regression offers a balance between simple (linear) models and complex (neural networks)."
      ],
      "metadata": {
        "id": "PNeEueB4nJPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "ANS:  Polynomial Regression is an extension of Linear Regression where the relationship between the independent variable and the dependent variable is modeled as an n-degree polynomial function.\n",
        "Y= b0 + b1X+b2X2+ b3X3+ bnXn\n",
        "where,\n",
        "Y= Dependent variable\n",
        "X = Independent variable\n",
        "b0, b1, b2, bn = Regression coefficients.\n",
        "n = Degree of the polynomial.\n"
      ],
      "metadata": {
        "id": "U2d3DP_SnwJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "ANS: Yes, polynomial regression can be applied to multiple variables! This is an extension of polynomial regression where the model includes polynomial terms not just for one independent variable, but for multiple variables and even their interactions."
      ],
      "metadata": {
        "id": "2fkfDUK3pe7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "ANS: Polynomial Regression is a powerful tool for modeling non-linear relationships, but it has several limitations, including overfitting, high computational cost, and difficulty in interpretation.\n",
        "- As the degree of the polynomial increases, the model becomes more flexible and may fit the training data very closely, including noise. This can lead to poor generalization to new data.\n",
        "- Polynomial regression performs poorly when predicting values outside the range of the training data. The curve can behave unpredictably at the extremes, producing unrealistic predictions.\n",
        "-With large datasets and high-degree polynomials, the complexity of computation increases significantly, making the model less efficient."
      ],
      "metadata": {
        "id": "gv17iJXzqH9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "ANS: Selecting the right polynomial degree is crucial to balance underfitting and overfitting. To evaluate model fit, we use statistical metrics, cross-validation, and residual analysis.\n",
        "- Examine the residuals for patterns.A good fit shows residuals randomly scattered around zero. If patterns persist, the polynomial degree might be too low (underfitting) or too high (overfitting).\n",
        "- Fit models with different polynomial degrees on the training set, then evaluate their performance on the testing set. A model that performs well on both sets is optimal.\n",
        "- Use metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or Root Mean Squared Error (RMSE) to quantify model performance."
      ],
      "metadata": {
        "id": "z5yZPXNYqiPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "ANS: Visualization important in polynomial regression to understand the model's performance, detect underfitting or overfitting, and ensure the selected polynomial degree captures the data's true trend.\n",
        "- Polynomial regression models nonlinear relationships, and visualization allows you to see how well the curve fits the data points.It can reveal whether the chosen degree of the polynomial accurately captures the underlying trend.\n",
        "- A curve that is too wiggly indicates the model is overfitting, capturing noise instead of the true pattern. A curve that is too simple indicates underfitting, missing key details in the relationship.\n",
        "- By visualizing the curve and data, you can interpret how the predictors influence the outcome variable. It becomes easier to see where the model performs well and where it struggles, especially at the extremes or in regions of sparse data."
      ],
      "metadata": {
        "id": "8AMd8jDHrHhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "ANS: Polynomial regression can be implemented in Python using the PolynomialFeatures from sklearn.preprocessing and LinearRegression from sklearn.linear_model. It involves transforming the input features to include polynomial terms and then applying linear regression to the transformed features.\n",
        "Steps to Implement Polynomial Regression:\n",
        "1) Import Required Libraries\n",
        "2) Generate Sample Data\n",
        "3) Apply Linear Regression (Baseline Model)\n",
        "4) Transform Data for Polynomial Regression\n",
        "5) Visualize Polynomial Regression Fit\n",
        "6) Compare Performance Using R² Score\n",
        "7) Automating Polynomial Regression for Different Degrees\n"
      ],
      "metadata": {
        "id": "_c8HcJs9rv3q"
      }
    }
  ]
}